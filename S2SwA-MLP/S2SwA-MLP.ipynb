{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ed666e",
   "metadata": {},
   "source": [
    "第一步：导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c0e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452c8eb",
   "metadata": {},
   "source": [
    "第二步：定义编码器（Encoder）\n",
    "编码器是由16个LSTM单元组成的循环神经网络，每个LSTM接收32维的输入向量序列，并输出128维的隐藏状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84f84722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        outputs, (hidden, cell) = self.lstm(src)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa37ca",
   "metadata": {},
   "source": [
    "第三步：定义注意力机制（Attention）\n",
    "注意力机制将编码器的输出和当前的解码器隐藏状态作为输入，计算当前解码器隐藏状态的注意力权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ff0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        hidden = hidden.repeat(src_len, 1, 1).transpose(0, 1)\n",
    "        \n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c158e5",
   "metadata": {},
   "source": [
    "第四步：定义解码器（Decoder）\n",
    "解码器也是由16个LSTM单元组成，使用来自注意力机制的加权上下文作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec249cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.lstm = nn.LSTM(hidden_dim + output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        lstm_input = torch.cat((input.unsqueeze(1), context_vector), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393caed8",
   "metadata": {},
   "source": [
    "第五步：定义MLP\n",
    "最后一层是一个多层感知机，用于从解码器的输出中生成最终的分类结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dd6067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71563d2b",
   "metadata": {},
   "source": [
    "组装整个模型\n",
    "现在我们有了所有必要的组件，可以组装成完整的模型。但是，请注意，根据你的描述，整个模型架构的细节可能需要根据实际任务进行调整。这个实现提供了一个框架，你可能需要根据具体需求进行微调和优化。以下是如何将上述组件组合成完整模型的示例：\n",
    "\n",
    "定义完整的Seq2Seq模型\n",
    "这个Seq2Seq模型将包含一个编码器、一个解码器、和一个注意力机制，以及一个多层感知机来处理解码器的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42aa57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention, mlp):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = attention\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src维度是 [batch_size, src_len, input_dim]\n",
    "        # trg维度是 [batch_size, trg_len, output_dim]\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # 解码器的第一个输入是目标序列的第一个元素，例如：<sos>标记\n",
    "        input = trg[:,0,:]\n",
    "\n",
    "        outputs = torch.zeros(trg.shape).to(trg.device)\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            # 将输入和前一个隐藏状态以及所有编码器输出传递给解码器\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # 将解码器的输出保存起来\n",
    "            outputs[:,t,:] = output\n",
    "            \n",
    "            # 决定是否使用真实的下一个输入或者是解码器的输出作为下一个输入\n",
    "            input = output\n",
    "        \n",
    "        # 将解码器的所有输出通过MLP来得到最终的输出\n",
    "        outputs = self.mlp(outputs.reshape(-1, outputs.shape[2]))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab484db",
   "metadata": {},
   "source": [
    "这个模型首先通过编码器处理源序列，然后逐步通过解码器处理目标序列的每个元素。每个解码器步骤都使用前一步的隐藏状态和注意力机制来关注编码器的输出。最终，解码器的输出被送入MLP以得到最终的分类结果。\n",
    "\n",
    "实例化模型和组件\n",
    "最后，我们需要实例化模型和所有的组件，并准备好模型训练所需的其他元素（如优化器、损失函数等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29bd25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 32\n",
    "OUTPUT_DIM = 2\n",
    "HIDDEN_DIM = 128\n",
    "ENC_LAYERS = 16\n",
    "DEC_LAYERS = 16\n",
    "MLP_HIDDEN_DIM = 128\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS)\n",
    "attention = Attention(HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, attention)\n",
    "mlp = MLP(HIDDEN_DIM, MLP_HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, attention, mlp)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# 定义损失函数，例如交叉熵\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b266dc6",
   "metadata": {},
   "source": [
    "请注意，这只是一个起点。构建有效的深度学习模型需要仔细地设计模型架构、调整参数、选择适当的优化器和学习率、以及进行充分的实验和验证。希望这个示例能为你构建自己的模型提供一个有用的参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c432f",
   "metadata": {},
   "source": [
    "基于你的需求，我将提供一个完整的例子，展示如何在PyTorch中加载数据集、训练模型并保存训练好的模型。这个例子将综合使用前面定义的编码器、解码器、注意力机制和多层感知机（MLP），适用于处理指定的数据集 (D={X,Y})，其中 (X={X_1, X_2, \\ldots, X_{16}}) 每个 (X_i) 是一个具有32维的向量，(Y={Y_1, Y_2}) 是通过one-hot编码表示的标签。\n",
    "\n",
    "导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a381b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61c964",
   "metadata": {},
   "source": [
    "加载数据集\n",
    "首先，我们需要创建一个Dataset类来加载你的数据集。这里，我们假设数据集已经以适当的格式准备好（例如，作为NumPy数组或者保存在文件中）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e34272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# 假设X和Y是你的数据和标签\n",
    "# X = torch.randn(1000, 16, 32)  # 生成随机数据作为示例\n",
    "# Y = torch.randint(0, 2, (1000, 2))  # 生成随机标签作为示例\n",
    "\n",
    "# dataset = CustomDataset(X, Y)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162437da",
   "metadata": {},
   "source": [
    "训练模型\n",
    "接下来，定义模型训练的函数。这里，我们假设model是你根据上述组件实例化的完整模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c45e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X, Y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X, Y)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5f5b7",
   "metadata": {},
   "source": [
    "保存模型\n",
    "最后，展示如何保存训练好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdaa0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path='model.pth'):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e604ca4",
   "metadata": {},
   "source": [
    "组装并运行\n",
    "将上述代码片段组装起来，并运行整个训练流程。确保在实际应用中替换掉生成随机数据和标签的部分，改为加载你的实际数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4e012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型、优化器和损失函数\n",
    "model = Seq2Seq(encoder, decoder, attention, mlp)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 调用训练函数\n",
    "# train_model(model, dataloader, optimizer, criterion, epochs=10)\n",
    "\n",
    "# 保存模型\n",
    "# save_model(model, 'my_seq2seq_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
